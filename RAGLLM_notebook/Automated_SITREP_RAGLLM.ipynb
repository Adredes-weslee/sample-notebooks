{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Automated SITREP Generator from Incident Logs (RAG-LLM)\n",
        "\n",
        "This notebook builds a **self-contained** Retrieval-Augmented Generation (RAG) pipeline that:\n",
        "- Downloads or ingests a **public MOT Marine accident investigation PDF** (with fallbacks).\n",
        "- Performs **robust PDF extraction** (page-anchored) and chunking.\n",
        "- Indexes chunks via **FAISS** + **sentence-transformers**.\n",
        "- Uses a **local, open-source LLM** for summarization with **guardrails** to avoid hallucination.\n",
        "- Implements **V&V**: hallucination probing and **Groundedness Score**.\n",
        "- Generates a **Streamlit app** (via `%%writefile app/app.py`) to interactively produce SITREPs with citations.\n",
        "- Saves governance artifacts to `artifacts/` (SPEC-DRIVE, model card, metrics, plots).\n",
        "\n",
        "> Blue Lane = low-code path (default). Red Lane = optional deep dives.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "361cb39d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python  : 3.11.13\n",
            "OS      : Windows-10-10.0.26200-SP0\n",
            "CPU     : Intel64 Family 6 Model 183 Stepping 1, GenuineIntel\n",
            "Torch   : 2.9.0+cpu\n",
            "CUDA    : False\n",
            "OFFLINE : False\n",
            "Working dir: c:\\Users\\tcmk_\\Downloads\\elice notebooks\\RAGLLM_notebook\n",
            "Created artifacts/model_card.md\n"
          ]
        }
      ],
      "source": [
        "# --- Bootstrap & Environment --------------------------------------------------\n",
        "import os, sys, random, json, time, pathlib, platform, shutil, subprocess, re\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "print(\"Python  :\", platform.python_version())\n",
        "print(\"OS      :\", platform.platform())\n",
        "print(\"CPU     :\", platform.processor())\n",
        "try:\n",
        "    import torch\n",
        "    has_torch = True\n",
        "    print(\"Torch   :\", torch.__version__)\n",
        "    print(\"CUDA    :\", torch.cuda.is_available())\n",
        "except Exception as e:\n",
        "    has_torch = False\n",
        "    print(\"Torch   : not installed\")\n",
        "\n",
        "# Deterministic seeds\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "if has_torch:\n",
        "    torch.manual_seed(SEED)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(SEED)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Create folders\n",
        "for d in [\"data\", \"models\", \"artifacts\", \"artifacts/plots\", \"app\", \"hf_cache\"]:\n",
        "    Path(d).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Flags\n",
        "BLUE_LANE = True            # low-code default\n",
        "SKIP_HEAVY_TRAINING = True  # keep runtime short\n",
        "\n",
        "# Offline detection\n",
        "OFFLINE = False\n",
        "try:\n",
        "    import socket\n",
        "    socket.create_connection((\"www.google.com\", 80), timeout=3)\n",
        "except Exception:\n",
        "    OFFLINE = True\n",
        "print(\"OFFLINE :\", OFFLINE)\n",
        "\n",
        "# Hugging Face cache\n",
        "os.environ[\"TRANSFORMERS_CACHE\"] = str(Path(\"hf_cache\").resolve())\n",
        "\n",
        "print(\"Working dir:\", os.getcwd())\n",
        "\n",
        "# Initialize model card with a data-use note placeholder\n",
        "model_card_path = Path(\"artifacts/model_card.md\")\n",
        "if not model_card_path.exists():\n",
        "    model_card_path.write_text(\n",
        "        \"# Model Card (WIP)\\n\\n\"\n",
        "        \"## Data Use & License\\n\"\n",
        "        \"- Source: Public MOT Marine accident investigation report (Singapore) or mirrored open sample.\\n\"\n",
        "        \"- No classified data. Educational/experimental use.\\n\\n\"\n",
        "        \"## Notes\\n\"\n",
        "        \"- This file will be appended with details after pipeline runs.\\n\",\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "print(\"Created artifacts/model_card.md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "35825e93",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using Python 3.11 at: c:\\Users\\tcmk_\\Downloads\\elice notebooks\\.venv311\\Scripts\\python.exe\n",
            "Now switch the Notebook kernel to: Python 3.11 (sitrep)\n"
          ]
        }
      ],
      "source": [
        "# Create venv with Python 3.11 (no 'py' launcher needed)\n",
        "import os, sys, subprocess, shutil, platform, pathlib\n",
        "\n",
        "def find_py311():\n",
        "    # 1) Prefer env var override\n",
        "    for env_var in [\"PYTHON311\", \"P311\", \"PY311\"]:\n",
        "        p = os.environ.get(env_var)\n",
        "        if p and pathlib.Path(p).exists():\n",
        "            return p\n",
        "    # 2) If current interpreter is 3.111, use it\n",
        "    if sys.version_info[:2] == (3, 11):  # fixed\n",
        "        return sys.executable\n",
        "    # 3) Try common Windows install paths\n",
        "    candidates = [\n",
        "        r\"C:\\Users\\%USERNAME%\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\",\n",
        "        r\"C:\\Program Files\\Python311\\python.exe\",\n",
        "        r\"C:\\Program Files (x86)\\Python311\\python.exe\",\n",
        "    ]\n",
        "    candidates = [os.path.expandvars(p) for p in candidates]\n",
        "    for p in candidates:\n",
        "        if pathlib.Path(p).exists():\n",
        "            return p\n",
        "    # 4) Try anything named python3.11 on PATH\n",
        "    p = shutil.which(\"python3.11\") or shutil.which(\"python311\")\n",
        "    if p:\n",
        "        return p\n",
        "    return None\n",
        "\n",
        "py311 = find_py311()\n",
        "if not py311:\n",
        "    print(\"[ERROR] Could not find Python 3.11 on this system.\")\n",
        "    print(\"Install it, then re-run this cell. From VS Code PowerShell:\")\n",
        "    print('  winget install -e --id Python.Python.3.11')\n",
        "else:\n",
        "    print(\"Using Python 3.11 at:\", py311)\n",
        "    # Create venv\n",
        "    subprocess.check_call([py311, \"-m\", \"venv\", \".venv311\"])\n",
        "    # Upgrade pip and ensure ipykernel in the venv\n",
        "    subprocess.check_call([r\".\\.venv311\\Scripts\\python\", \"-m\", \"pip\", \"install\", \"-U\", \"pip\", \"setuptools\", \"wheel\"])\n",
        "    subprocess.check_call([r\".\\.venv311\\Scripts\\python\", \"-m\", \"pip\", \"install\", \"ipykernel\"])\n",
        "    # Register Jupyter kernel\n",
        "    subprocess.check_call([r\".\\.venv311\\Scripts\\python\", \"-m\", \"ipykernel\", \"install\",\n",
        "                           \"--user\", \"--name\", \"sitrep311\", \"--display-name\", \"Python 3.11 (sitrep)\"])\n",
        "    print(\"Now switch the Notebook kernel to: Python 3.11 (sitrep)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4f0e3a6b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing numpy ...\n",
            "Installing pymupdf ...\n",
            "Installing faiss-cpu ...\n",
            "Installing scikit-learn ...\n",
            "Installing langchain ...\n",
            "Installing transformers ...\n",
            "Installing accelerate ...\n",
            "Installing sentence-transformers ...\n",
            "Installing streamlit ...\n",
            "Installing spacy ...\n"
          ]
        }
      ],
      "source": [
        "# --- Install Packages (graceful) ----------------------------------------------\n",
        "# Run this AFTER switching kernel to \"Python 3.11 (sitrep)\"\n",
        "import sys, subprocess\n",
        "\n",
        "def pip_install(pkgs):\n",
        "    for p in pkgs:\n",
        "        try:\n",
        "            print(f\"Installing {p} ...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", p])\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Could not install {p}: {e}\")\n",
        "\n",
        "required = [\n",
        "    \"numpy\",\n",
        "    \"pymupdf\",               # PDF extraction\n",
        "    \"faiss-cpu\",             # vector store\n",
        "    \"scikit-learn\",\n",
        "    \"langchain\",             # splitters/util\n",
        "    \"transformers\",  # LLMs\n",
        "    \"accelerate\",\n",
        "    \"sentence-transformers\",\n",
        "    \"streamlit\",             # app\n",
        "    \"spacy\"                  # optional PII redaction\n",
        "]\n",
        "pip_install(required)\n",
        "\n",
        "# Optional CPU torch\n",
        "try:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "                           \"torch\", \"--index-url\", \"https://download.pytorch.org/whl/cpu\"])\n",
        "except Exception as e:\n",
        "    print(\"[WARN] torch CPU wheel install failed:\", e)\n",
        "\n",
        "# Optional: small spaCy model\n",
        "try:\n",
        "    import spacy\n",
        "    try:\n",
        "        spacy.load(\"en_core_web_sm\")\n",
        "    except Exception:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "except Exception as e:\n",
        "    print(\"[WARN] spaCy optional model unavailable:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "209770bc",
      "metadata": {},
      "source": [
        "## SPEC-DRIVE Card (Fill Me)\n",
        "\n",
        "- **Mission:** *(What decision does the SITREP support?)*\n",
        "- **Inputs:** *(One PDF incident report from public MOT Marine accident investigations)*\n",
        "- **Outputs:** *(Concise SITREP with citations + groundedness score)*\n",
        "- **Constraints:** *(No classified data; use only provided document; answer `NOT FOUND IN SOURCE` if missing)*\n",
        "- **V&V Success Criteria:** *(Groundedness ≥ 0.90; 0 unsupported claims)*\n",
        "- **Rollback Plan:** *(If groundedness < target, tighten retrieval, reduce generation temperature, or fall back to extractive)*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "18c350c1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved artifacts/spec_drive.md\n"
          ]
        }
      ],
      "source": [
        "# --- Save SPEC-DRIVE to artifacts/spec_drive.md -------------------------------\n",
        "# Edit the SPEC_CARD text and run this cell.\n",
        "SPEC_CARD = \"\"\"\n",
        "Mission: <edit me>\n",
        "Inputs: MOT Marine incident PDF\n",
        "Outputs: SITREP with citations + groundedness score\n",
        "Constraints: No classified data; use only provided document; answer NOT FOUND IN SOURCE if missing\n",
        "V&V Success Criteria: Groundedness >= 0.90; 0 unsupported claims\n",
        "Rollback Plan: Tighten retrieval; reduce temperature; extractive fallback if needed\n",
        "\"\"\"\n",
        "\n",
        "from pathlib import Path\n",
        "Path(\"artifacts/spec_drive.md\").write_text(SPEC_CARD.strip()+\"\\n\", encoding=\"utf-8\")\n",
        "print(\"Saved artifacts/spec_drive.md\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a3e36d8b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using existing report: data\\report.pdf\n",
            "PDF pages: 24\n",
            "First 1000 chars of extracted text:\n",
            " Final Report \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "Missing of Fitter  \n",
            "From SRS RTM Zheng He \n",
            "At Sea \n",
            "On 26 December 2024 \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "TIB/MAI/CAS.187 \n",
            " \n",
            "Transport Safety Investigation Bureau \n",
            "Ministry of Transport \n",
            "Singapore \n",
            " \n",
            "18 July 2025 \n",
            "\n",
            " \n",
            "© 2025 Government of Singapore  \n",
            "ii \n",
            " \n",
            "The Transport Safety Investigation Bureau of Singapore \n",
            "The Transport Safety Investigation Bureau of Singapore (TSIB) is the air, marine \n",
            "and rail accidents and incidents investigation authority in Singapore. Its mission is to \n",
            "promote transport safety through the conduct of independent investigations into air, \n",
            "marine and rail accidents and incidents. \n",
            "TSIB conducts marine safety investigations in accordance with the Singapore \n",
            "Transport Safety Investigations Act 2018, Transport Safety Investigations (Marine \n",
            "Occurrences) Regulations 2023 and the Casualty Investigation Code under SOLAS \n",
            "Regulation XI-1/6 adopted by the International Maritime Organization (IMO) Resolution \n",
            "MSC 255(84). \n",
            "The sole objective of TSIB’s marine safety investi\n"
          ]
        }
      ],
      "source": [
        "# --- Data Ingestion & PDF Extraction ------------------------------------------\n",
        "import os, json, io\n",
        "from pathlib import Path\n",
        "\n",
        "# Use this specific public report (preferred)\n",
        "REPORT_URL = \"https://www.mot.gov.sg/docs/default-source/about-mot/missing-of-fitter-from-rtm-zheng-he-at-sea-on-26-december-2024.pdf?sfvrsn=b4c661aa_1\"\n",
        "SAMPLE_PDF_URLS = [REPORT_URL]\n",
        "\n",
        "pdf_path = Path(\"data/report.pdf\")\n",
        "pdf_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def is_pdf_bytes(content: bytes, headers: dict) -> bool:\n",
        "    ct = (headers or {}).get(\"content-type\", \"\").lower()\n",
        "    return (content.startswith(b\"%PDF\") or \"application/pdf\" in ct or ct.endswith(\"/pdf\"))\n",
        "\n",
        "def try_download_pdf(urls, out_path: Path) -> bool:\n",
        "    import requests\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0\",\n",
        "        \"Accept\": \"application/pdf,application/octet-stream,*/*;q=0.8\",\n",
        "    }\n",
        "    for u in urls:\n",
        "        try:\n",
        "            print(\"Attempting:\", u)\n",
        "            r = requests.get(u, timeout=45, headers=headers)\n",
        "            if r.status_code == 200 and r.content and is_pdf_bytes(r.content, r.headers):\n",
        "                out_path.write_bytes(r.content)\n",
        "                print(\"Downloaded:\", out_path)\n",
        "                return True\n",
        "            else:\n",
        "                print(\"[WARN] Not a PDF or empty response:\", r.status_code, r.headers.get(\"content-type\"))\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Download failed:\", e)\n",
        "    return False\n",
        "\n",
        "def create_synthetic_pdf(out_path: Path) -> bool:\n",
        "    \"\"\"Create a tiny synthetic PDF as last fallback.\"\"\"\n",
        "    try:\n",
        "        from reportlab.pdfgen import canvas\n",
        "        from reportlab.lib.pagesizes import letter\n",
        "        c = canvas.Canvas(str(out_path), pagesize=letter)\n",
        "        text = c.beginText(40, 750); text.setFont(\"Helvetica\", 11)\n",
        "        for line in [\n",
        "            \"Marine Incident Report - Synthetic Sample\",\n",
        "            \"Date: 2020-08-14\",\n",
        "            \"Vessel: MV Example Star (Singapore-flagged)\",\n",
        "            \"Location: Approaches to Singapore Strait\",\n",
        "            \"Summary: At 03:41 LT, steering responded sluggishly while entering congested waters.\",\n",
        "            \"Actions: Engine telegraph set to slow ahead; navigational warning issued.\",\n",
        "            \"Damage: Minor scrape port bow, no injuries reported.\",\n",
        "            \"Findings: Fatigue among bridge team; incomplete checklist use.\",\n",
        "        ]:\n",
        "            text.textLine(line)\n",
        "        c.drawText(text); c.showPage(); c.save()\n",
        "        print(\"Created synthetic PDF:\", out_path)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Could not generate synthetic PDF:\", e)\n",
        "        return False\n",
        "\n",
        "# 1) Reuse existing local report if present (skip download)\n",
        "if pdf_path.exists() and pdf_path.stat().st_size > 0:\n",
        "    print(\"Using existing report:\", pdf_path)\n",
        "else:\n",
        "    ok = False\n",
        "    if not OFFLINE:\n",
        "        ok = try_download_pdf(SAMPLE_PDF_URLS, pdf_path)\n",
        "    if not ok and not pdf_path.exists():\n",
        "        ok = create_synthetic_pdf(pdf_path)\n",
        "\n",
        "if not pdf_path.exists():\n",
        "    raise FileNotFoundError(\"No PDF available. Please upload a file to data/report.pdf and re-run.\")\n",
        "\n",
        "# Extract text with page anchors via PyMuPDF\n",
        "import fitz  # PyMuPDF\n",
        "doc = fitz.open(str(pdf_path))\n",
        "page_texts = []\n",
        "for i, page in enumerate(doc):\n",
        "    text = page.get_text(\"text\")\n",
        "    page_texts.append({\"page\": i+1, \"text\": text})\n",
        "doc.close()\n",
        "\n",
        "# Save page-anchored chunks for transparency\n",
        "Path(\"data/report_chunks.json\").write_text(json.dumps(page_texts, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "full_text = \"\\n\".join([p[\"text\"] for p in page_texts]).strip()\n",
        "\n",
        "print(\"PDF pages:\", len(page_texts))\n",
        "print(\"First 1000 chars of extracted text:\\n\", full_text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7cdbeaf7",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\tcmk_\\Downloads\\elice notebooks\\.venv311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "c:\\Users\\tcmk_\\Downloads\\elice notebooks\\.venv311\\Lib\\site-packages\\transformers\\utils\\hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FAISS index built with 50 chunks. Saved to models/faiss.index\n"
          ]
        }
      ],
      "source": [
        "# --- Retrieval Pipeline: Chunking, Embeddings, FAISS --------------------------\n",
        "from pathlib import Path\n",
        "import json, re\n",
        "import numpy as np\n",
        "\n",
        "# Try LangChain splitter; fallback to simple splitter if unavailable\n",
        "try:\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "    def chunk_text(txt, chunk_size=1000, overlap=200):\n",
        "        splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
        "        return splitter.split_text(txt)\n",
        "except Exception:\n",
        "    def chunk_text(txt, chunk_size=1000, overlap=200):\n",
        "        chunks = []\n",
        "        start = 0\n",
        "        while start < len(txt):\n",
        "            end = min(len(txt), start + chunk_size)\n",
        "            chunks.append(txt[start:end])\n",
        "            start = end - overlap if end - overlap > start else end\n",
        "        return chunks\n",
        "\n",
        "chunks = chunk_text(full_text, chunk_size=1000, overlap=200)\n",
        "\n",
        "Path(\"data/chunks.json\").write_text(json.dumps(chunks, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "\n",
        "# Map chunk_id -> a representative page (best-effort by position)\n",
        "chunk_map = {}\n",
        "if page_texts:\n",
        "    page_concat = [p[\"text\"] for p in page_texts]\n",
        "    cum = [0]\n",
        "    for t in page_concat:\n",
        "        cum.append(cum[-1] + len(t))\n",
        "    for i, ch in enumerate(chunks):\n",
        "        idx = full_text.find(ch[:50]) if ch else -1\n",
        "        if idx < 0:\n",
        "            idx = full_text.find(ch[:20]) if ch else -1\n",
        "        if idx < 0:\n",
        "            idx = 0\n",
        "        for p in range(1, len(cum)):\n",
        "            if idx < cum[p]:\n",
        "                chunk_map[i] = {\"page\": p, \"preview\": ch[:120] if ch else \"\"}\n",
        "                break\n",
        "else:\n",
        "    for i, ch in enumerate(chunks):\n",
        "        chunk_map[i] = {\"page\": None, \"preview\": ch[:120] if ch else \"\"}\n",
        "\n",
        "# Embeddings (safe import + fallback)\n",
        "EMBED_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "embed_model = None\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer  # import inside try to avoid hard crash\n",
        "    embed_model = SentenceTransformer(EMBED_MODEL_NAME)\n",
        "    embeddings = embed_model.encode(\n",
        "        chunks, batch_size=32, show_progress_bar=False, convert_to_numpy=True, normalize_embeddings=True\n",
        "    ).astype(np.float32)\n",
        "except Exception as e:\n",
        "    print(\"[WARN] SentenceTransformer unavailable; using TF-IDF instead. Reason:\", e)\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.preprocessing import normalize as sk_normalize\n",
        "    tfidf = TfidfVectorizer(max_features=4096)\n",
        "    embeddings = tfidf.fit_transform(chunks).astype(np.float32).toarray()\n",
        "    embeddings = sk_normalize(embeddings, norm=\"l2\").astype(np.float32)  # cosine via IP\n",
        "\n",
        "# FAISS index\n",
        "index = None\n",
        "try:\n",
        "    if len(chunks) == 0:\n",
        "        raise ValueError(\"No text chunks extracted; skipping FAISS index build.\")\n",
        "    import faiss\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)  # cosine via normalized inner product\n",
        "    index.add(embeddings.astype(np.float32))\n",
        "    Path(\"models\").mkdir(parents=True, exist_ok=True)\n",
        "    faiss.write_index(index, \"models/faiss.index\")\n",
        "    with open(\"data/chunk_map.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(chunk_map, f, ensure_ascii=False, indent=2)\n",
        "    print(\"FAISS index built with\", len(chunks), \"chunks. Saved to models/faiss.index\")\n",
        "except Exception as e:\n",
        "    print(\"[WARN] Could not build FAISS index:\", e)\n",
        "    index = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ca7ba940",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded LLM: google/flan-t5-small\n"
          ]
        }
      ],
      "source": [
        "# --- Generation (Local-first with guardrails) ---------------------------------\n",
        "from typing import List, Tuple, Dict\n",
        "import re, math, json\n",
        "\n",
        "GUARDRAIL_INSTRUCTION = (\n",
        "    \"You are generating a concise SITREP using ONLY the provided context. \"\n",
        "    \"If the requested information is not present in the context, respond exactly: NOT FOUND IN SOURCE. \"\n",
        "    \"Do not use outside knowledge.\"\n",
        ")\n",
        "\n",
        "# Safe import: transformers may be broken due to env mismatch\n",
        "try:\n",
        "    from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "except Exception as e:\n",
        "    print(\"[WARN] transformers unavailable; will use extractive fallback. Reason:\", e)\n",
        "    AutoTokenizer = AutoModelForSeq2SeqLM = pipeline = None\n",
        "\n",
        "LLM_NAME_CANDIDATES = [\n",
        "    \"google/flan-t5-small\",\n",
        "    \"sshleifer/distilbart-cnn-12-6\"\n",
        "]\n",
        "\n",
        "tokenizer = None\n",
        "seq2seq = None\n",
        "pipe = None\n",
        "if pipeline is not None:\n",
        "    for name in LLM_NAME_CANDIDATES:\n",
        "        try:\n",
        "            tokenizer = AutoTokenizer.from_pretrained(name)\n",
        "            seq2seq = AutoModelForSeq2SeqLM.from_pretrained(name)\n",
        "            pipe = pipeline(\"text2text-generation\", model=seq2seq, tokenizer=tokenizer)\n",
        "            print(\"Loaded LLM:\", name)\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(\"[WARN] Could not load\", name, \"->\", e)\n",
        "\n",
        "# Reuse retrieval for extractive baseline, with robust TF-IDF fallback\n",
        "def retrieve_top_k(query: str, k: int = 5) -> List[int]:\n",
        "    import numpy as np\n",
        "    # If FAISS + embedding model available, use them\n",
        "    if (index is not None) and (embed_model is not None):\n",
        "        q_emb = embed_model.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
        "        D, I = index.search(q_emb.astype(np.float32), k)\n",
        "        return I[0].tolist()\n",
        "    # TF-IDF fallback (does not need transformers/accelerate)\n",
        "    try:\n",
        "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "        from sklearn.metrics.pairwise import cosine_similarity\n",
        "        vec = TfidfVectorizer(max_features=4096)\n",
        "        X = vec.fit_transform(chunks)\n",
        "        q = vec.transform([query])\n",
        "        sims = cosine_similarity(q, X).ravel()\n",
        "        return np.argsort(-sims)[:k].tolist()\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] TF-IDF retrieval failed:\", e)\n",
        "        return list(range(min(k, len(chunks))))\n",
        "\n",
        "def _preview(text: str, n: int = 200) -> str:\n",
        "    return text[:n].replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
        "\n",
        "def extractive_sitrep(query: str, top_k: int = 5) -> Tuple[str, List[int]]:\n",
        "    top_idx = retrieve_top_k(query, k=top_k)\n",
        "    selected = [chunks[i] for i in top_idx]\n",
        "    lines = [\"- \" + _preview(s) + \"...\" for s in selected]\n",
        "    sitrep = \"SITREP (Extractive Baseline)\\n\" + \"\\n\".join(lines)\n",
        "    return sitrep, top_idx\n",
        "\n",
        "def generate_baseline(prompt: str, max_tokens: int = 256) -> str:\n",
        "    if pipe is None:\n",
        "        s, _ = extractive_sitrep(prompt, top_k=5)\n",
        "        return s\n",
        "    out = pipe(prompt, max_new_tokens=max_tokens, do_sample=False)[0][\"generated_text\"]  # fixed\n",
        "    return out\n",
        "\n",
        "def generate_rag(prompt: str, top_k: int = 5, max_tokens: int = 256) -> Tuple[str, Dict[int, str]]:\n",
        "    top_idx = retrieve_top_k(prompt, top_k)\n",
        "    context_parts = []\n",
        "    for i in top_idx:\n",
        "        page = chunk_map.get(i, {}).get('page')\n",
        "        preview = chunks[i]\n",
        "        context_parts.append(f\"[Chunk {i} | Page {page}]\\n{preview}\")\n",
        "    context = \"\\n\\n\".join(context_parts)\n",
        "    rag_prompt = (\n",
        "        f\"{GUARDRAIL_INSTRUCTION}\\n\"\n",
        "        f\"Context:\\n{context}\\n\\n\"\n",
        "        f\"User request: {prompt}\\n\"\n",
        "        \"Produce a concise, structured SITREP. Where info is missing, write: NOT FOUND IN SOURCE.\\n\"\n",
        "    )\n",
        "    if pipe is None:\n",
        "        s, _ = extractive_sitrep(prompt, top_k=top_k)\n",
        "        citations = {i: chunk_map.get(i, {'page': None, 'preview': chunks[i][:120]}) for i in top_idx}\n",
        "        return s, citations\n",
        "\n",
        "    out = pipe(rag_prompt, max_new_tokens=max_tokens, do_sample=False)[0][\"generated_text\"]\n",
        "    citations = {i: chunk_map.get(i, {'page': None, 'preview': chunks[i][:120]}) for i in top_idx}\n",
        "    return out, citations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "6fabd68a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PII redaction ready (toggle MASK_PII=True to activate).\n"
          ]
        }
      ],
      "source": [
        "# --- Optional PII Redaction (Governance Demo) ---------------------------------\n",
        "MASK_PII = False\n",
        "\n",
        "def redact_pii(text: str, use_spacy: bool = True) -> str:\n",
        "    if not MASK_PII:\n",
        "        return text\n",
        "    try:\n",
        "        import spacy\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "        doc = nlp(text)\n",
        "        redacted = text\n",
        "        # Replace from end to start to not offset spans\n",
        "        for ent in sorted(doc.ents, key=lambda e: e.start_char, reverse=True):\n",
        "            if ent.label_ in {\"PERSON\",\"ORG\"}:\n",
        "                redacted = redacted[:ent.start_char] + \"[REDACTED]\" + redacted[ent.end_char:]\n",
        "        return redacted\n",
        "    except Exception:\n",
        "        # Simple fallback: naive proper noun masking (very rough)\n",
        "        import re\n",
        "        return re.sub(r\"\\b([A-Z][a-z]+\\s[A-Z][a-z]+)\\b\", \"[REDACTED]\", text)\n",
        "\n",
        "print(\"PII redaction ready (toggle MASK_PII=True to activate).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ded5b9d9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Probe] Unanswerable question (Baseline vs RAG)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (634 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Baseline response:\n",
            " edward\n",
            "\n",
            "RAG response:\n",
            " [Chunk 34 | Page 17] [Chunk 34 | Page 17] 1.6 The other crew qualifications and roles 1.6.1 RZH had a total of 25 crew9 including the Master at the time of the occurrence. 9 12 from India, 12 from the Philippines, one from Ukraine [Chunk 3 | Page 3]  2025 Government of Singapore iii Table of Contents ABBREVIATIONS iv SYNOPSIS 1 VIEW OF VESSEL 2 1 Factual information 3 1.1 Narrative 3 1.2 The vessel was in port in January 2025. 1.8.3 The Company shared that they were also implementing guidelines \n",
            "\n",
            "[Probe] Answerable SITREP (RAG)\n",
            "[Chunk 3 | Page 2]  2025 Government of Singapore iii Table of Contents ABBREVIATIONS iv SYNOPSIS 1 VIEW OF VESSEL 2 1 Factual information 3 1.1 Narrative 3 1.2 The vessel 10 1.3 TSIB conducts marine safety investigations in accordance with the Singapore Transport Safety Investigations Act 2018, Transport Safety Investigations (Marine Occurrences) Regulations 2023 and the Casualty Investigation Code under SOLAS Regulation XI-1/6 adopted by the International Maritime Organization (IMO) Resolution MSC 255(84). The sole objective of TSIB’s marine safety investigations is the prevention of marine a\n",
            "Groundedness: 1.00\n",
            "Saved artifacts/vnv_report.md\n"
          ]
        }
      ],
      "source": [
        "# --- V&V: Hallucination Probes & Groundedness ---------------------------------\n",
        "import json, re\n",
        "from pathlib import Path\n",
        "\n",
        "def lcs_length(a_tokens, b_tokens):\n",
        "    # Longest Common Subsequence length (for rough ROUGE-L recall)\n",
        "    dp = [[0]*(len(b_tokens)+1) for _ in range(len(a_tokens)+1)]\n",
        "    for i in range(1, len(a_tokens)+1):\n",
        "        for j in range(1, len(b_tokens)+1):\n",
        "            if a_tokens[i-1] == b_tokens[j-1]:\n",
        "                dp[i][j] = dp[i-1][j-1] + 1\n",
        "            else:\n",
        "                dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
        "    return dp[-1][-1]\n",
        "\n",
        "def tokenize_simple(s):\n",
        "    return re.findall(r\"[A-Za-z0-9]+\", s.lower())\n",
        "\n",
        "def sentence_split(s):\n",
        "    return [x.strip() for x in re.split(r\"[\\n\\.;!?]+\", s) if x.strip()]\n",
        "\n",
        "def groundedness_score(sitrep: str, retrieved_indices: list, jaccard_tau=0.2, rougeL_tau=0.2):\n",
        "    # Build candidate source tokens\n",
        "    source_text = \"\\n\\n\".join([chunks[i] for i in retrieved_indices])\n",
        "    source_tokens = tokenize_simple(source_text)\n",
        "    sents = sentence_split(sitrep)\n",
        "    supported = []\n",
        "    for sent in sents:\n",
        "        stoks = tokenize_simple(sent)\n",
        "        if not stoks:\n",
        "            supported.append(False); continue\n",
        "        # Jaccard vs source\n",
        "        inter = len(set(stoks) & set(source_tokens))\n",
        "        union = len(set(stoks) | set(source_tokens)) or 1\n",
        "        jacc = inter/union\n",
        "        # ROUGE-L approx via LCS recall\n",
        "        lcs = lcs_length(stoks, source_tokens)\n",
        "        rougeL = lcs / (len(stoks) or 1)\n",
        "        ok = (jacc >= jaccard_tau) or (rougeL >= rougeL_tau) or (\"not found in source\" in sent.lower())\n",
        "        supported.append(ok)\n",
        "    coverage = sum(supported) / (len(supported) or 1)\n",
        "    return coverage, list(zip(sents, supported))\n",
        "\n",
        "# --- Run probes ---\n",
        "UNANSWERABLE = \"What was the captain's middle name?\"\n",
        "ANSWERABLE   = \"Generate a concise SITREP covering date/time, vessel, location, actions, damage and findings.\"\n",
        "\n",
        "print(\"\\n[Probe] Unanswerable question (Baseline vs RAG)\")\n",
        "b_resp = generate_baseline(UNANSWERABLE)\n",
        "r_resp, r_cites = generate_rag(UNANSWERABLE, top_k=5)\n",
        "print(\"\\nBaseline response:\\n\", b_resp[:500])\n",
        "print(\"\\nRAG response:\\n\", r_resp[:500])\n",
        "\n",
        "print(\"\\n[Probe] Answerable SITREP (RAG)\")\n",
        "sitrep_text, cites = generate_rag(ANSWERABLE, top_k=5)\n",
        "print(sitrep_text[:600])\n",
        "\n",
        "# Compute groundedness vs the retrieved chunks used for SITREP\n",
        "retrieved_idx = list(cites.keys())\n",
        "g_score, details = groundedness_score(sitrep_text, retrieved_idx)\n",
        "\n",
        "artifacts = {\n",
        "    \"groundedness\": {\n",
        "        \"score\": g_score,\n",
        "        \"details\": [{\"sentence\": s, \"supported\": bool(ok)} for s, ok in details],\n",
        "        \"retrieved_indices\": retrieved_idx,\n",
        "        \"citations\": cites\n",
        "    }\n",
        "}\n",
        "Path(\"artifacts/groundedness.json\").write_text(json.dumps(artifacts, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "print(f\"Groundedness: {g_score:.2f}\")\n",
        "\n",
        "# Write a human-readable V&V report\n",
        "with open(\"artifacts/vnv_report.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"# V&V Report\\n\\n\")\n",
        "    f.write(f\"- Groundedness Score: {g_score:.2f}\\n\")\n",
        "    f.write(\"- Unsupported sentences listed below (if any):\\n\")\n",
        "    for s, ok in details:\n",
        "        if not ok:\n",
        "            f.write(f\"  - {s}\\n\")\n",
        "    f.write(\"\\n## Citations (retrieved chunks)\\n\")\n",
        "    for i, meta in cites.items():\n",
        "        f.write(f\"- Chunk {i}, Page {meta.get('page')}: {meta.get('preview','')[:160]}\\n\")\n",
        "print(\"Saved artifacts/vnv_report.md\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "3c3d5bbc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Appended details to artifacts/model_card.md\n"
          ]
        }
      ],
      "source": [
        "# --- Append to Model Card -----------------------------------------------------\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "card_path = Path(\"artifacts/model_card.md\")\n",
        "appendix = f\"\"\"\n",
        "## Pipeline Details (Appended {datetime.datetime.utcnow().isoformat()}Z)\n",
        "- PDF: data/report.pdf\n",
        "- Extraction: PyMuPDF (page-anchored)\n",
        "- Retrieval: sentence-transformers/all-MiniLM-L6-v2 + FAISS (fallback: TF-IDF)\n",
        "- Generator: google/flan-t5-small or distilbart-cnn (fallback: extractive)\n",
        "- Guardrails: context-only; 'NOT FOUND IN SOURCE' for missing info\n",
        "- Governance: Hallucination probe + Groundedness Score (artifacts/groundedness.json, artifacts/vnv_report.md)\n",
        "- Streamlit app: app/app.py (cached loaders)\n",
        "\n",
        "## Known Failure Modes\n",
        "- Long or image-heavy PDFs may yield poor text extraction.\n",
        "- LLM may truncate if prompt exceeds context length.\n",
        "- Offline environments trigger extractive fallback (reduced quality).\n",
        "\n",
        "## Next Steps\n",
        "- Add OCR for scanned PDFs.\n",
        "- Enrich retrieval with metadata (sections, headings).\n",
        "- Human-in-the-loop verification workflow.\n",
        "\"\"\"\n",
        "card_path.write_text(card_path.read_text(encoding=\"utf-8\") + \"\\n\" + appendix, encoding=\"utf-8\")\n",
        "print(\"Appended details to artifacts/model_card.md\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9fd79452",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote app/app.py\n"
          ]
        }
      ],
      "source": [
        "# --- Streamlit App (file writer) ----------------------------------------------\n",
        "from pathlib import Path\n",
        "\n",
        "app_code = r\"\"\"\n",
        "import os, json, re, time\n",
        "from pathlib import Path\n",
        "import streamlit as st\n",
        "\n",
        "st.set_page_config(page_title=\"Automated SITREP (RAG-LLM)\", layout=\"wide\")\n",
        "\n",
        "@st.cache_resource\n",
        "def load_artifacts():\n",
        "    t0 = time.perf_counter()\n",
        "\n",
        "    # Load chunked text used by retrieval/FAISS (NOT pages)\n",
        "    chunks = []\n",
        "    try:\n",
        "        chunks = json.loads(Path(\"data/chunks.json\").read_text(encoding=\"utf-8\"))\n",
        "    except Exception:\n",
        "        # Fallback to page texts if chunks.json is missing\n",
        "        try:\n",
        "            pages = json.loads(Path(\"data/report_chunks.json\").read_text(encoding=\"utf-8\"))\n",
        "            chunks = [p[\"text\"] for p in pages]\n",
        "            st.warning(\"data/chunks.json missing; using page texts. Retrieval may be misaligned with FAISS.\")\n",
        "        except Exception:\n",
        "            st.error(\"No chunks available. Please re-run the notebook pipeline.\")\n",
        "            chunks = []\n",
        "\n",
        "    # Chunk map (for page numbers + previews)\n",
        "    try:\n",
        "        chunk_map = json.loads(Path(\"data/chunk_map.json\").read_text(encoding=\"utf-8\"))\n",
        "    except Exception:\n",
        "        chunk_map = {i: {\"page\": None, \"preview\": (chunks[i][:120] if i < len(chunks) else \"\")}\n",
        "                     for i in range(len(chunks))}\n",
        "\n",
        "    t_after_io = time.perf_counter()\n",
        "\n",
        "    # Embeddings\n",
        "    embed_model = None\n",
        "    try:\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "        embed_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    except Exception as e:\n",
        "        st.warning(f\"Embedding model load failed: {e}\")\n",
        "\n",
        "    t_after_embed = time.perf_counter()\n",
        "\n",
        "    # FAISS\n",
        "    index = None\n",
        "    index_size = 0\n",
        "    try:\n",
        "        import faiss, numpy as np  # noqa: F401\n",
        "        index = faiss.read_index(\"models/faiss.index\")\n",
        "        index_size = index.ntotal\n",
        "    except Exception as e:\n",
        "        st.warning(f\"FAISS index not available: {e}\")\n",
        "\n",
        "    t_after_faiss = time.perf_counter()\n",
        "\n",
        "    # LLM\n",
        "    pipe = None\n",
        "    try:\n",
        "        from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "        for name in [\"sshleifer/distilbart-cnn-12-6\", \"google/flan-t5-small\"]:\n",
        "            try:\n",
        "                tok = AutoTokenizer.from_pretrained(name)\n",
        "                mdl = AutoModelForSeq2SeqLM.from_pretrained(name)\n",
        "                pipe = pipeline(\"text2text-generation\", model=mdl, tokenizer=tok)\n",
        "                st.info(f\"Loaded LLM: {name}\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                st.warning(f\"Could not load {name}: {e}\")\n",
        "    except Exception as e:\n",
        "        st.warning(f\"Transformers not available: {e}\")\n",
        "        pipe = None\n",
        "\n",
        "    t_after_llm = time.perf_counter()\n",
        "\n",
        "    # Timers\n",
        "    st.info(\n",
        "        f\"Init timings (s) | IO: {t_after_io - t0:.2f} | \"\n",
        "        f\"Embeds: {t_after_embed - t_after_io:.2f} | \"\n",
        "        f\"FAISS: {t_after_faiss - t_after_embed:.2f} | \"\n",
        "        f\"LLM: {t_after_llm - t_after_faiss:.2f} | \"\n",
        "        f\"Total: {t_after_llm - t0:.2f}\"\n",
        "    )\n",
        "\n",
        "    # Mismatch warning\n",
        "    if index is not None and len(chunks) and index_size and index_size != len(chunks):\n",
        "        st.warning(f\"Mismatch: FAISS ntotal={index_size} but chunks={len(chunks)}. \"\n",
        "                   f\"Re-run the retrieval cell to regenerate data/chunks.json and models/faiss.index together.\")\n",
        "\n",
        "    return chunks, chunk_map, embed_model, index, pipe\n",
        "\n",
        "chunks, chunk_map, embed_model, index, pipe = load_artifacts()\n",
        "\n",
        "GUARDRAIL_INSTRUCTION = (\n",
        "    \"You are generating a concise SITREP using ONLY the provided context. \"\n",
        "    \"If the requested information is not present in the context, respond exactly: NOT FOUND IN SOURCE. \"\n",
        "    \"Do not use outside knowledge.\"\n",
        ")\n",
        "\n",
        "def retrieve_top_k(query: str, k: int = 5):\n",
        "    if not chunks:\n",
        "        return []\n",
        "    k = max(1, min(k, len(chunks)))\n",
        "    if index is None or embed_model is None:\n",
        "        return list(range(k))\n",
        "    import numpy as np, faiss  # noqa: F401\n",
        "    q_emb = embed_model.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
        "    D, I = index.search(q_emb.astype(np.float32), k)\n",
        "    top = [int(i) for i in I[0].tolist() if isinstance(i, (int, np.integer))]\n",
        "    top = [i for i in top if 0 <= i < len(chunks)]\n",
        "    if not top:\n",
        "        top = list(range(k))\n",
        "    return top\n",
        "\n",
        "def generate_rag(prompt: str, top_k: int = 5, max_tokens: int = 256):\n",
        "    top_idx = retrieve_top_k(prompt, top_k)\n",
        "    if not top_idx:\n",
        "        return \"No chunks available to generate a SITREP.\", {}\n",
        "\n",
        "    # Build context safely (chunk_map keys may be strings)\n",
        "    ctx_parts, valid_idx = [], []\n",
        "    for i in top_idx:\n",
        "        meta = (chunk_map.get(str(i)) or chunk_map.get(i) or {})\n",
        "        page = meta.get(\"page\")\n",
        "        ctx_parts.append(f\"[Chunk {i} | Page {page}]\\\\n{chunks[i]}\")\n",
        "        valid_idx.append(i)\n",
        "\n",
        "    ctx = \"\\\\n\\\\n\".join(ctx_parts)\n",
        "    rag_prompt = (\n",
        "        f\"{GUARDRAIL_INSTRUCTION}\\\\n\"\n",
        "        f\"Context:\\\\n{ctx}\\\\n\\\\n\"\n",
        "        f\"User request: {prompt}\\\\n\"\n",
        "        \"Produce a concise, structured SITREP. Where info is missing, write: NOT FOUND IN SOURCE.\\\\n\"\n",
        "    )\n",
        "    if pipe is None:\n",
        "        selected = [chunks[i][:200].replace(\"\\\\n\", \" \") for i in valid_idx]\n",
        "        out = \"SITREP (Extractive Fallback)\\\\n\" + \"\\\\n\".join([f\"- {s}...\" for s in selected])\n",
        "    else:\n",
        "        out = pipe(rag_prompt, max_new_tokens=max_tokens, do_sample=False, truncation=True)[0][\"generated_text\"]\n",
        "\n",
        "    citations = {i: (chunk_map.get(str(i)) or chunk_map.get(i) or {\"page\": None, \"preview\": chunks[i][:120]})\n",
        "                 for i in valid_idx}\n",
        "    return out, citations\n",
        "\n",
        "st.title(\"Automated SITREP Generator (RAG-LLM)\")\n",
        "\n",
        "left, right = st.columns([1,1])\n",
        "with left:\n",
        "    st.subheader(\"Source Report Text\")\n",
        "    try:\n",
        "        pages = json.loads(Path(\"data/report_chunks.json\").read_text(encoding=\"utf-8\"))\n",
        "        for p in pages:\n",
        "            with st.expander(f\"Page {p.get('page')}\"):\n",
        "                st.write(p.get(\"text\",\"\"))\n",
        "    except Exception:\n",
        "        st.info(\"No pages available. Please run the notebook pipeline first.\")\n",
        "\n",
        "with right:\n",
        "    st.subheader(\"Generate SITREP\")\n",
        "    prompt_choice = st.selectbox(\"Sample prompts\", [\n",
        "        \"Generate a concise SITREP covering date/time, vessel, location, actions, damage and findings.\",\n",
        "        \"List key findings as bullet points.\",\n",
        "        \"Create an incident timeline.\"\n",
        "    ])\n",
        "    custom_prompt = st.text_area(\"Or write your own prompt\", value=\"\", placeholder=\"Type here to override the sample prompt...\")\n",
        "    TOP_K = st.slider(\"TOP_K (retrieved chunks)\", min_value=3, max_value=10, value=5, step=1)\n",
        "    mask_pii = st.checkbox(\"Mask PII (simple NER)\", value=False)\n",
        "    if st.button(\"Generate SITREP\"):\n",
        "        # Custom prompt takes precedence if non-empty after stripping\n",
        "        prompt = custom_prompt.strip() or prompt_choice\n",
        "        st.caption(f\"Using prompt: {prompt!r}\")\n",
        "        out, cites = generate_rag(prompt, top_k=TOP_K)\n",
        "        if mask_pii:\n",
        "            try:\n",
        "                import spacy\n",
        "                nlp = spacy.load(\"en_core_web_sm\")\n",
        "                doc = nlp(out)\n",
        "                out_red = out\n",
        "                for ent in sorted(doc.ents, key=lambda e: e.start_char, reverse=True):\n",
        "                    if ent.label_ in {\"PERSON\",\"ORG\"}:\n",
        "                        out_red = out_red[:ent.start_char] + \"[REDACTED]\" + out_red[ent.end_char:]\n",
        "                out = out_red\n",
        "            except Exception as e:\n",
        "                st.warning(f\"PII masking unavailable: {e}\")\n",
        "        st.markdown(\"### SITREP\")\n",
        "        st.write(out)\n",
        "\n",
        "        st.markdown(\"### Citations\")\n",
        "        for i, meta in cites.items():\n",
        "            with st.expander(f\"Chunk {i} (Page {meta.get('page')})\"):\n",
        "                st.write(meta.get(\"preview\",\"\"))\n",
        "\n",
        "# Save a small run summary\n",
        "try:\n",
        "    Path(\"artifacts\").mkdir(exist_ok=True, parents=True)\n",
        "    Path(\"artifacts/run_summary.json\").write_text(json.dumps({\n",
        "        \"ts\": __import__(\"datetime\").datetime.utcnow().isoformat()+\"Z\",\n",
        "        \"chunks\": len(chunks),\n",
        "        \"index_loaded\": bool(index is not None),\n",
        "        \"llm_loaded\": bool(pipe is not None)\n",
        "    }, indent=2), encoding=\"utf-8\")\n",
        "except Exception as e:\n",
        "    st.warning(f\"Could not write run summary: {e}\")\n",
        "\"\"\"\n",
        "\n",
        "Path(\"app\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"app/app.py\").write_text(app_code, encoding=\"utf-8\")\n",
        "print(\"Wrote app/app.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a474c1a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Launching: c:\\Users\\tcmk_\\Downloads\\elice notebooks\\.venv311\\Scripts\\python.exe -m streamlit run app/app.py --server.address localhost --server.port 8501 --browser.serverAddress localhost --browser.serverPort 8501\n",
            "Open: http://localhost:8501/\n"
          ]
        }
      ],
      "source": [
        "import os, sys, subprocess\n",
        "\n",
        "port = os.environ.get(\"PORT\", \"8501\")\n",
        "address = os.environ.get(\"BIND_ADDR\", \"localhost\")  # ensure clickable link uses localhost\n",
        "cmd = [\n",
        "    sys.executable, \"-m\", \"streamlit\", \"run\", \"app/app.py\",\n",
        "    \"--server.address\", address,\n",
        "    \"--server.port\", str(port),\n",
        "    \"--browser.serverAddress\", \"localhost\",\n",
        "    \"--browser.serverPort\", str(port),\n",
        "]\n",
        "print(\"Launching:\", \" \".join(cmd))\n",
        "print(f\"Open: http://localhost:{port}/\")\n",
        "# This will hold the cell until you stop Streamlit (Ctrl+C in terminal/kernel).\n",
        "subprocess.run(cmd, check=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4f42fac",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
