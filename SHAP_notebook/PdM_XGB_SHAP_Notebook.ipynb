{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predictive Maintenance for Critical Platform Machinery (ML-Classification + Explainability)\n",
        "\n",
        "This self-contained notebook:\n",
        "- Trains a **tree-based classifier (XGBoost)** on an open Industrial IoT Fault Detection dataset (Kaggle), with fallbacks.\n",
        "- Performs governance-grade **V&V**: train vs test, **PR-AUC**, **ROC-AUC**, confusion matrix, **calibration** (ECE), and **threshold tuning**.\n",
        "- Explains predictions with **SHAP** (global + local).\n",
        "- Builds a **Streamlit diagnostic app** with sliders and a **live SHAP** waterfall.\n",
        "- Saves SPEC-DRIVE, metrics, calibration plots, SHAP plots, and a model card to `artifacts/`.\n",
        "\n",
        "> Archetype: **ML-Classification**. (Optional **Red Lane** adds a sequence model if timestamps exist.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Bootstrap & Environment --------------------------------------------------\n",
        "import os, sys, random, json, time, pathlib, platform, shutil, subprocess, re, socket, pickle\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "print(\"Python  :\", platform.python_version())\n",
        "print(\"OS      :\", platform.platform())\n",
        "print(\"CPU     :\", platform.processor())\n",
        "\n",
        "# Deterministic seeds\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "\n",
        "# Create folders\n",
        "for d in [\"data\", \"models\", \"artifacts\", \"artifacts/plots\", \"app\"]:\n",
        "    Path(d).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Flags\n",
        "BLUE_LANE = True\n",
        "SKIP_HEAVY_TRAINING = False  # XGBoost training is lightweight; keep False by default\n",
        "\n",
        "# Offline detection\n",
        "OFFLINE = False\n",
        "try:\n",
        "    socket.create_connection((\"www.google.com\", 80), timeout=3)\n",
        "except Exception:\n",
        "    OFFLINE = True\n",
        "print(\"OFFLINE :\", OFFLINE)\n",
        "\n",
        "# Initialize model card with a data-use note placeholder\n",
        "model_card = Path(\"artifacts/model_card.md\")\n",
        "if not model_card.exists():\n",
        "    model_card.write_text(\n",
        "        \"# Model Card (WIP)\\n\\n\"\n",
        "        \"## Data Use & License\\n\"\n",
        "        \"- Source: Industrial IoT Fault Detection dataset (Kaggle) if available; otherwise mirrored sample / user upload.\\n\"\n",
        "        \"- Data used for educational experimentation; **no classified data**.\\n\"\n",
        "        \"## Notes\\n\"\n",
        "        \"- This file will be appended with preprocessing, calibration results, and explainability findings.\\n\",\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "print(\"Created/updated artifacts/model_card.md\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Install Packages (graceful) ----------------------------------------------\n",
        "def pip_install(pkgs):\n",
        "    import subprocess, sys\n",
        "    for p in pkgs:\n",
        "        try:\n",
        "            print(f\"Installing {p} ...\")\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", p])\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Could not install {p}: {e}\")\n",
        "\n",
        "required = [\n",
        "    \"pandas\",\n",
        "    \"numpy\",\n",
        "    \"scikit-learn\",\n",
        "    \"xgboost\",\n",
        "    \"shap\",\n",
        "    \"matplotlib\",\n",
        "    \"seaborn\",\n",
        "    \"streamlit\",\n",
        "    \"opendatasets\"\n",
        "]\n",
        "pip_install(required)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SPEC-DRIVE Card (Fill Me)\n",
        "\n",
        "- **Engineering Requirement:** *(e.g., predict pump failure ≥ 1 hour ahead)*\n",
        "- **Input Data (Sensors):** *(vibration, temperature, pressure, ...)*\n",
        "- **Prediction Task:** *(binary fault classification)*\n",
        "- **Diagnostic Requirement:** *(top contributing sensors per prediction via SHAP)*\n",
        "- **V&V Targets:** *(PR-AUC ≥ 0.80, ECE ≤ 0.05, acceptable FP rate)*\n",
        "- **Rollback Plan:** *(threshold tuning, recalibration, more data, sequence model)*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Save SPEC-DRIVE to artifacts/spec_drive.md -------------------------------\n",
        "SPEC_CARD = \"\"\"\n",
        "Engineering Requirement: <edit me>\n",
        "Input Data (Sensors): vibration, temperature, pressure\n",
        "Prediction Task: binary fault classification\n",
        "Diagnostic Requirement: top contributing sensors per prediction via SHAP\n",
        "V&V Targets: PR-AUC ≥ 0.80, ECE ≤ 0.05\n",
        "Rollback Plan: threshold tuning; recalibration; more data; optional sequence model\n",
        "\"\"\"\n",
        "Path(\"artifacts/spec_drive.md\").write_text(SPEC_CARD.strip()+\"\\n\", encoding=\"utf-8\")\n",
        "print(\"Saved artifacts/spec_drive.md\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Data Acquisition & Loading ----------------------------------------------\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_PATH = Path(\"data/iot_faults.csv\")\n",
        "KAGGLE_DIR = Path(\"data/kaggle\")\n",
        "\n",
        "def try_kaggle_download():\n",
        "    # Skip Kaggle if local file already exists\n",
        "    if DATA_PATH.exists():\n",
        "        print(\"Local data file exists, skipping Kaggle download:\", DATA_PATH)\n",
        "        return None\n",
        "\n",
        "    # Reuse existing Kaggle CSV if already downloaded\n",
        "    existing_csvs = list(KAGGLE_DIR.rglob(\"*.csv\"))\n",
        "    if existing_csvs:\n",
        "        p = existing_csvs[0]\n",
        "        print(\"Using existing Kaggle CSV:\", p)\n",
        "        return pd.read_csv(p)\n",
        "\n",
        "    # If offline, skip\n",
        "    if 'OFFLINE' in globals() and OFFLINE:\n",
        "        print(\"Offline detected, skipping Kaggle download.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        import opendatasets as od\n",
        "        url = \"https://www.kaggle.com/datasets/ziya07/industrial-iot-fault-detection-dataset\"\n",
        "        print(\"Attempting Kaggle download via opendatasets ...\")\n",
        "        KAGGLE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "        od.download(url, data_dir=str(KAGGLE_DIR))\n",
        "        # Heuristic: find a csv\n",
        "        for p in KAGGLE_DIR.rglob(\"*.csv\"):\n",
        "            print(\"Found CSV:\", p)\n",
        "            return pd.read_csv(p)\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Kaggle/opendatasets unavailable:\", e)\n",
        "    return None\n",
        "\n",
        "def try_mirror_download():\n",
        "    # Public mirror fallback (may fail offline). Replace with a known reachable URL if available.\n",
        "    mirror_url = \"https://raw.githubusercontent.com/plotly/datasets/master/diabetes.csv\"\n",
        "    # NOTE: This is a placeholder CSV with similar numeric structure; we will relabel for demo if used.\n",
        "    try:\n",
        "        print(\"Attempting mirrored CSV download ...\", mirror_url)\n",
        "        return pd.read_csv(mirror_url)\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Mirror download failed:\", e)\n",
        "        return None\n",
        "\n",
        "def last_resort_tiny_synthetic(n=800, seed=42):\n",
        "    print(\"[INFO] Creating tiny synthetic dataset as last resort.\")\n",
        "    import numpy as np\n",
        "    rng = np.random.default_rng(seed)\n",
        "    temp = rng.normal(80, 10, n)        # Temperature\n",
        "    vib  = rng.normal(1.0, 0.4, n)      # Vibration\n",
        "    press= rng.normal(5.0, 1.0, n)      # Pressure\n",
        "    # Fault probability increases with high temp, high vib, abnormal pressure\n",
        "    logits = (temp-80)/10 + (vib-1.0)/0.4 + (np.abs(press-5.0))/(0.8)\n",
        "    probs = 1/(1+np.exp(-logits/2.5))\n",
        "    y = (rng.uniform(0,1,n) < probs*0.4).astype(int)\n",
        "    df = pd.DataFrame({\"Temperature\":temp, \"Vibration\":vib, \"Pressure\":press, \"Fault\":y})\n",
        "    return df\n",
        "\n",
        "df = None\n",
        "if DATA_PATH.exists():\n",
        "    print(\"Loading local upload:\", DATA_PATH)\n",
        "    df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "if df is None and not OFFLINE:\n",
        "    df = try_kaggle_download()\n",
        "\n",
        "if df is None and not OFFLINE:\n",
        "    df = try_mirror_download()\n",
        "\n",
        "if df is None:\n",
        "    df = last_resort_tiny_synthetic()\n",
        "\n",
        "# Harmonize column names to expected schema\n",
        "original_cols = list(df.columns)\n",
        "df.columns = [c.strip().replace(\" \", \"_\").title() for c in df.columns]\n",
        "# If mirrored diabetes.csv used, adapt\n",
        "if \"Outcome\" in df.columns and \"Fault\" not in df.columns:\n",
        "    df = df.rename(columns={\"Outcome\":\"Fault\"})\n",
        "# Ensure 'Fault' exists\n",
        "if \"Fault\" not in df.columns:\n",
        "    # Heuristic: create Fault from top quartile of first numeric feature\n",
        "    num_cols = df.select_dtypes(include=\"number\").columns.tolist()\n",
        "    if not num_cols:\n",
        "        raise ValueError(\"No numeric columns found to derive a label from.\")\n",
        "    thresh = df[num_cols[0]].quantile(0.75)\n",
        "    df[\"Fault\"] = (df[num_cols[0]] >= thresh).astype(int)\n",
        "\n",
        "print(\"Columns (original -> normalized):\", original_cols, \"->\", list(df.columns))\n",
        "print(\"Shape:\", df.shape)\n",
        "display(df.head())\n",
        "class_balance = df[\"Fault\"].value_counts(normalize=True).to_dict()\n",
        "print(\"Class balance:\", class_balance)\n",
        "\n",
        "# Save a copy of the working dataset for reproducibility\n",
        "Path(\"data/working.csv\").write_text(df.to_csv(index=False), encoding=\"utf-8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Preprocessing & Splits ---------------------------------------------------\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Choose features: all numeric except label-like columns\n",
        "num_cols = df.select_dtypes(include=\"number\").columns.tolist()\n",
        "\n",
        "# Normalize possible label names to 'Fault'\n",
        "if \"Fault_Label\" in df.columns and \"Fault\" not in df.columns:\n",
        "    df = df.rename(columns={\"Fault_Label\": \"Fault\"})\n",
        "if \"Label\" in df.columns and \"Fault\" not in df.columns:\n",
        "    df = df.rename(columns={\"Label\": \"Fault\"})\n",
        "if \"Target\" in df.columns and \"Fault\" not in df.columns:\n",
        "    df = df.rename(columns={\"Target\": \"Fault\"})\n",
        "\n",
        "target_col = \"Fault\"\n",
        "EXCLUDE_KEYS = {\"fault\", \"label\", \"target\", \"class\", \"y\"}\n",
        "feature_cols = [c for c in num_cols if c != target_col and not any(k in c.lower() for k in EXCLUDE_KEYS)]\n",
        "\n",
        "X = df[feature_cols].copy()\n",
        "y = df[target_col].astype(int).copy()\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=SEED, stratify=y\n",
        ")\n",
        "\n",
        "# Standardize\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "# Persist scaler + meta\n",
        "import joblib, json\n",
        "Path(\"models\").mkdir(exist_ok=True, parents=True)\n",
        "joblib.dump(scaler, \"models/scaler.pkl\")\n",
        "feature_meta = {\n",
        "    \"feature_cols\": feature_cols,\n",
        "    \"ranges\": {c: {\"min\": float(X[c].min()), \"max\": float(X[c].max()), \"mean\": float(X[c].mean())} for c in feature_cols}\n",
        "}\n",
        "Path(\"models/features_meta.json\").write_text(json.dumps(feature_meta, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "# Save SHAP background from training data\n",
        "bg_idx = np.random.choice(len(X_train_scaled), size=min(300, len(X_train_scaled)), replace=False)\n",
        "bg = X_train_scaled[bg_idx]\n",
        "np.save(\"models/shap_bg.npy\", bg)\n",
        "\n",
        "print(\"Features:\", feature_cols)\n",
        "print(\"Train/Test shapes:\", X_train.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Modeling (Blue Lane) -----------------------------------------------------\n",
        "from xgboost import XGBClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Handle class imbalance\n",
        "pos_weight = (len(y_train) - y_train.sum()) / max(1, y_train.sum())\n",
        "pos_weight = float(np.clip(pos_weight, 0.5, 10.0))\n",
        "\n",
        "model = XGBClassifier(\n",
        "    n_estimators=300,\n",
        "    max_depth=4,\n",
        "    learning_rate=0.05,\n",
        "    subsample=0.9,\n",
        "    colsample_bytree=0.9,\n",
        "    reg_lambda=1.0,\n",
        "    random_state=SEED,\n",
        "    n_jobs=2,\n",
        "    eval_metric=\"logloss\",\n",
        "    scale_pos_weight=pos_weight\n",
        ")\n",
        "\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Persist model\n",
        "import joblib\n",
        "joblib.dump(model, \"models/xgb.pkl\")\n",
        "print(\"Saved models/xgb.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Metrics & V&V ------------------------------------------------------------\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score, confusion_matrix\n",
        ")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def compute_metrics(model, Xs, ys, split_name):\n",
        "    proba = model.predict_proba(Xs)[:,1]\n",
        "    preds = (proba >= 0.5).astype(int)\n",
        "    metrics = {\n",
        "        \"split\": split_name,\n",
        "        \"accuracy\": float(accuracy_score(ys, preds)),\n",
        "        \"precision\": float(precision_score(ys, preds, zero_division=0)),\n",
        "        \"recall\": float(recall_score(ys, preds)),\n",
        "        \"f1\": float(f1_score(ys, preds)),\n",
        "        \"roc_auc\": float(roc_auc_score(ys, proba)),\n",
        "        \"pr_auc\": float(average_precision_score(ys, proba))\n",
        "    }\n",
        "    cm = confusion_matrix(ys, preds).tolist()\n",
        "    metrics[\"confusion_matrix\"] = cm\n",
        "    return metrics, proba, preds\n",
        "\n",
        "train_metrics, train_proba, train_preds = compute_metrics(model, X_train_scaled, y_train, \"train\")\n",
        "test_metrics,  test_proba,  test_preds  = compute_metrics(model, X_test_scaled,  y_test,  \"test\")\n",
        "\n",
        "all_metrics = {\"train\": train_metrics, \"test\": test_metrics}\n",
        "Path(\"artifacts/metrics.json\").write_text(json.dumps(all_metrics, indent=2), encoding=\"utf-8\")\n",
        "print(json.dumps(all_metrics, indent=2))\n",
        "\n",
        "# Confusion matrix plot (test)\n",
        "plt.figure(figsize=(4,3))\n",
        "sns.heatmap(confusion_matrix(y_test, (test_proba>=0.5).astype(int)), annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix (Test @0.5)\"); plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
        "plt.tight_layout(); plt.savefig(\"artifacts/plots/confusion_matrix.png\"); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Calibration & ECE --------------------------------------------------------\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "def expected_calibration_error(probs, y_true, n_bins=10):\n",
        "    bins = np.linspace(0.0, 1.0, n_bins+1)\n",
        "    ece = 0.0\n",
        "    for i in range(n_bins):\n",
        "        lo, hi = bins[i], bins[i+1]\n",
        "        mask = (probs >= lo) & (probs < hi)\n",
        "        if mask.sum() == 0:\n",
        "            continue\n",
        "        acc = y_true[mask].mean()\n",
        "        conf = probs[mask].mean()\n",
        "        ece += (mask.mean()) * abs(acc - conf)\n",
        "    return float(ece)\n",
        "\n",
        "prob_true, prob_pred = calibration_curve(y_test, test_proba, n_bins=10, strategy='uniform')\n",
        "\n",
        "ece = expected_calibration_error(test_proba, y_test.values, n_bins=10)\n",
        "Path(\"artifacts/ece.json\").write_text(json.dumps({\"ece\": ece, \"bins\": 10}, indent=2), encoding=\"utf-8\")\n",
        "print(\"ECE:\", ece)\n",
        "\n",
        "# Reliability plot\n",
        "plt.figure(figsize=(4,3))\n",
        "plt.plot(prob_pred, prob_true, marker=\"o\", label=\"Model\")\n",
        "plt.plot([0,1],[0,1],\"--\", color=\"gray\", label=\"Perfectly calibrated\")\n",
        "plt.xlabel(\"Predicted probability\"); plt.ylabel(\"Fraction of positives\")\n",
        "plt.title(\"Reliability Diagram (Test)\")\n",
        "plt.legend(); plt.tight_layout()\n",
        "plt.savefig(\"artifacts/calibration_curve.png\"); plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Threshold tuning ---------------------------------------------------------\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "prec, rec, thr = precision_recall_curve(y_test, test_proba)\n",
        "# Choose threshold that maximizes F1\n",
        "f1s = 2*(prec*rec)/(prec+rec+1e-9)\n",
        "best_idx = int(np.nanargmax(f1s))\n",
        "best_thr = float(thr[max(0, best_idx-1)]) if len(thr)>0 else 0.5\n",
        "\n",
        "threshold_policy = {\n",
        "    \"chosen_threshold\": best_thr,\n",
        "    \"criterion\": \"max_F1\",\n",
        "    \"precision_at_thr\": float(prec[best_idx]),\n",
        "    \"recall_at_thr\": float(rec[best_idx])\n",
        "}\n",
        "Path(\"artifacts/threshold.json\").write_text(json.dumps(threshold_policy, indent=2), encoding=\"utf-8\")\n",
        "print(\"Threshold policy:\", threshold_policy)\n",
        "\n",
        "# Update V&V report\n",
        "with open(\"artifacts/vnv_report.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"# V&V Summary\\n\\n\")\n",
        "    f.write(\"## Metrics\\n\")\n",
        "    f.write(json.dumps({\"train\": train_metrics, \"test\": test_metrics}, indent=2))\n",
        "    f.write(\"\\n\\n## Calibration\\n\")\n",
        "    f.write(f\"- ECE: {threshold_policy['precision_at_thr'] if 'ece' not in locals() else ece}\\n\")\n",
        "    f.write(\"\\n## Threshold\\n\")\n",
        "    f.write(json.dumps(threshold_policy, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Explainability (SHAP) ----------------------------------------------------\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Background sample for SHAP speed\n",
        "bg_idx = np.random.choice(len(X_train_scaled), size=min(200, len(X_train_scaled)), replace=False)\n",
        "bg = X_train_scaled[bg_idx]\n",
        "\n",
        "explainer = shap.TreeExplainer(model, feature_perturbation=\"interventional\", data=bg)\n",
        "shap_values_test = explainer.shap_values(X_test_scaled)  # for TreeExplainer + XGBClassifier this returns array\n",
        "\n",
        "# Global importance (mean |shap|)\n",
        "mean_abs = np.abs(shap_values_test).mean(axis=0)\n",
        "order = np.argsort(mean_abs)[::-1]\n",
        "top_features = [feature_cols[i] for i in order[:10]]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(range(len(top_features)), mean_abs[order[:10]])\n",
        "plt.xticks(range(len(top_features)), top_features, rotation=45, ha=\"right\")\n",
        "plt.title(\"Global SHAP Feature Importance (|mean shap|)\")\n",
        "plt.tight_layout(); plt.savefig(\"artifacts/plots/shap_importance.png\"); plt.show()\n",
        "\n",
        "# Local explanation: pick a positive prediction if available\n",
        "pos_idx = np.where((test_proba>=best_thr) & (y_test.values==1))[0]\n",
        "idx = int(pos_idx[0]) if len(pos_idx)>0 else 0\n",
        "x0 = X_test_scaled[idx:idx+1]\n",
        "\n",
        "try:\n",
        "    shap_val0 = explainer.shap_values(x0)\n",
        "    base_val = explainer.expected_value\n",
        "    # Waterfall plot\n",
        "    shap.plots._waterfall.waterfall_legacy(base_val, shap_val0[0], feature_names=feature_cols, max_display=10, show=False)\n",
        "    plt.tight_layout(); plt.savefig(\"artifacts/plots/shap_waterfall.png\"); plt.show()\n",
        "except Exception as e:\n",
        "    print(\"[WARN] Waterfall plot failed, fallback to bar:\", e)\n",
        "    contrib = shap_val0[0]\n",
        "    order0 = np.argsort(np.abs(contrib))[::-1][:10]\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.bar(range(len(order0)), contrib[order0])\n",
        "    plt.xticks(range(len(order0)), [feature_cols[i] for i in order0], rotation=45, ha=\"right\")\n",
        "    plt.title(\"Local SHAP Contributions (fallback)\")\n",
        "    plt.tight_layout(); plt.savefig(\"artifacts/plots/shap_waterfall.png\"); plt.show()\n",
        "\n",
        "# Append a brief explanation to the V&V report\n",
        "with open(\"artifacts/vnv_report.md\", \"a\", encoding=\"utf-8\") as f:\n",
        "    f.write(\"\\n\\n## Prediction Explanation Report (Template)\\n\")\n",
        "    f.write(\"- Selected test sample index: %d\\n\" % idx)\n",
        "    f.write(\"- Model predicted a fault probability of %.3f at threshold %.3f.\\n\" % (float(test_proba[idx]), best_thr))\n",
        "    f.write(\"- Key contributing sensors (global): \" + \", \".join(top_features[:3]) + \"\\n\")\n",
        "    f.write(\"- Interpretation: High values in top features increased failure risk.\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c33a755",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Append to Model Card -----------------------------------------------------\n",
        "import datetime, json\n",
        "from pathlib import Path\n",
        "\n",
        "# Read metrics and calibration\n",
        "metrics = json.loads(Path(\"artifacts/metrics.json\").read_text()) if Path(\"artifacts/metrics.json\").exists() else {}\n",
        "ece_obj = json.loads(Path(\"artifacts/ece.json\").read_text()) if Path(\"artifacts/ece.json\").exists() else {}\n",
        "thr_obj = json.loads(Path(\"artifacts/threshold.json\").read_text()) if Path(\"artifacts/threshold.json\").exists() else {}\n",
        "\n",
        "appendix = f\"\"\"\n",
        "## Pipeline Details (Appended {datetime.datetime.utcnow().isoformat()}Z)\n",
        "- Dataset: Kaggle Industrial IoT Fault Detection (or mirrored/upload fallback).\n",
        "- Preprocessing: numeric standardization; stratified split 80/20.\n",
        "- Model: XGBoost (scale_pos_weight for imbalance).\n",
        "\n",
        "## V&V Summary\n",
        "- Metrics: {json.dumps(metrics)}\n",
        "- Calibration (ECE): {json.dumps(ece_obj)}\n",
        "- Threshold policy: {json.dumps(thr_obj)}\n",
        "\n",
        "## Explainability\n",
        "- Global SHAP importance: artifacts/plots/shap_importance.png\n",
        "- Local SHAP example: artifacts/plots/shap_waterfall.png\n",
        "\n",
        "## Limitations & Risks\n",
        "- Potential dataset shift; periodic revalidation required.\n",
        "- Threshold may require tuning per platform/mission.\n",
        "\"\"\"\n",
        "card = Path(\"artifacts/model_card.md\")\n",
        "card.write_text(card.read_text(encoding=\"utf-8\") + \"\\n\" + appendix, encoding=\"utf-8\")\n",
        "print(\"Appended details to artifacts/model_card.md\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Red Lane (Optional: Sequence Model)\n",
        "\n",
        "If your dataset has timestamps and true sequences per asset, consider a sliding-window **LSTM/Temporal-CNN** to capture temporal dynamics. Steps:\n",
        "\n",
        "1. Construct fixed-length windows per asset (e.g., last 60 readings → predict next-fault).\n",
        "2. Train a small LSTM for a few epochs.\n",
        "3. Compare **PR-AUC** and **calibration** with the XGBoost benchmark.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "Path(\"app\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "APP_PY = r\"\"\"\n",
        "import os, json\n",
        "from pathlib import Path\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "\n",
        "st.set_page_config(page_title=\"PdM: ML Classification + Explainability\", layout=\"wide\")\n",
        "\n",
        "@st.cache_resource\n",
        "def load_assets():\n",
        "    model = joblib.load(\"models/xgb.pkl\")\n",
        "    scaler = joblib.load(\"models/scaler.pkl\")\n",
        "    meta = json.loads(Path(\"models/features_meta.json\").read_text())\n",
        "    threshold = 0.5\n",
        "    thr_path = Path(\"artifacts/threshold.json\")\n",
        "    if thr_path.exists():\n",
        "        threshold = json.loads(thr_path.read_text()).get(\"chosen_threshold\", 0.5)\n",
        "    # Prefer saved training background; fallback to means\n",
        "    bg_path = Path(\"models/shap_bg.npy\")\n",
        "    if bg_path.exists():\n",
        "        bg = np.load(bg_path)\n",
        "    else:\n",
        "        means = np.array([meta[\"ranges\"][c][\"mean\"] for c in meta[\"feature_cols\"]]).reshape(1, -1)\n",
        "        bg = scaler.transform(np.repeat(means, repeats=200, axis=0))\n",
        "    explainer = shap.TreeExplainer(\n",
        "        model, feature_perturbation=\"interventional\", data=bg, model_output=\"probability\"\n",
        "    )\n",
        "    return model, scaler, meta, float(threshold), explainer\n",
        "\n",
        "model, scaler, meta, threshold, explainer = load_assets()\n",
        "feature_cols = meta[\"feature_cols\"]\n",
        "\n",
        "st.title(\"Predictive Maintenance — Diagnostic Console\")\n",
        "st.caption(\"ML-Classification (XGBoost) with live SHAP explainability.\")\n",
        "\n",
        "# Key feature list (guard against label leakage)\n",
        "try:\n",
        "    importances = model.feature_importances_\n",
        "    order = np.argsort(importances)[::-1]\n",
        "    key_cols = [feature_cols[i] for i in order[:min(6, len(feature_cols))]]\n",
        "except Exception:\n",
        "    key_cols = feature_cols[:min(6, len(feature_cols))]\n",
        "key_cols = [c for c in key_cols if \"fault\" not in c.lower()]\n",
        "\n",
        "def compute_and_store(values: dict, thr_val: float):\n",
        "    x = np.array([[values.get(c, meta[\"ranges\"][c][\"mean\"]) for c in feature_cols]])\n",
        "    x_scaled = scaler.transform(x)\n",
        "    proba = float(model.predict_proba(x_scaled)[0, 1])\n",
        "    pred = (proba >= thr_val)\n",
        "    try:\n",
        "        sv = explainer.shap_values(x_scaled, check_additivity=False)\n",
        "        if isinstance(sv, list):\n",
        "            sv_vec = sv[1][0] if len(sv) > 1 else sv[0][0]\n",
        "            base_val = explainer.expected_value[1] if np.ndim(explainer.expected_value) else explainer.expected_value\n",
        "        else:\n",
        "            sv_vec = sv[0]\n",
        "            base_val = explainer.expected_value\n",
        "        base_val = float(np.array(base_val).ravel()[0])\n",
        "        sv_vec = np.nan_to_num(np.array(sv_vec, dtype=float), nan=0.0, posinf=1e3, neginf=-1e3)\n",
        "        ex = shap.Explanation(values=sv_vec, base_values=base_val, data=x_scaled[0], feature_names=feature_cols)\n",
        "        fig, _ = plt.subplots(figsize=(7, 4))\n",
        "        shap.plots.waterfall(ex, max_display=10, show=False)\n",
        "        ax = plt.gca()\n",
        "        ax.xaxis.set_major_formatter(FuncFormatter(lambda x, pos: f\"{x:.3g}\"))\n",
        "        shap_fig = fig\n",
        "    except Exception as e:\n",
        "        shap_fig = (\"fallback\", f\"SHAP waterfall unavailable: {e}\")\n",
        "    st.session_state[\"last_result\"] = {\"proba\": proba, \"thr\": float(thr_val), \"pred\": bool(pred), \"shap_fig\": shap_fig}\n",
        "\n",
        "with st.sidebar:\n",
        "    with st.form(\"inputs_form\"):\n",
        "        st.header(\"Input Sensors\")\n",
        "        values = {}\n",
        "        for c in key_cols:\n",
        "            r = meta[\"ranges\"][c]\n",
        "            lo, hi, mu = float(r[\"min\"]), float(r[\"max\"]), float(r[\"mean\"])\n",
        "            span = hi - lo if hi > lo else (abs(mu) * 0.2 + 1.0)\n",
        "            lo2 = lo - 0.1 * span\n",
        "            hi2 = hi + 0.1 * span\n",
        "            values[c] = st.slider(c, float(lo2), float(hi2), float(mu), step=float(span/100 if span>0 else 0.1))\n",
        "        st.markdown(\"---\")\n",
        "        st.write(\"**Decision Threshold**\")\n",
        "        thr = st.slider(\"Threshold\", 0.05, 0.95, float(threshold), 0.01)\n",
        "        submitted = st.form_submit_button(\"Predict Status\")\n",
        "    if st.button(\"Clear Prediction\"):\n",
        "        st.session_state.pop(\"last_result\", None)\n",
        "\n",
        "if submitted:\n",
        "    compute_and_store(values, thr)\n",
        "\n",
        "if \"last_result\" in st.session_state:\n",
        "    res = st.session_state[\"last_result\"]\n",
        "    badge = \"✅ NORMAL\" if not res[\"pred\"] else \"⚠️ IMMINENT FAILURE\"\n",
        "    st.subheader(f\"Status: {badge}\")\n",
        "    st.write(f\"Probability: {res['proba']:.3f}  •  Threshold: {res['thr']:.2f}\")\n",
        "    shap_fig = res[\"shap_fig\"]\n",
        "    if isinstance(shap_fig, tuple) and shap_fig and shap_fig[0] == \"fallback\":\n",
        "        st.warning(shap_fig[1])\n",
        "    elif shap_fig is not None:\n",
        "        st.write(\"### SHAP Waterfall (local explanation)\")\n",
        "        st.pyplot(shap_fig)\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.download_button(\n",
        "    \"Download V&V Report\",\n",
        "    data=Path(\"artifacts/vnv_report.md\").read_text() if Path(\"artifacts/vnv_report.md\").exists() else \"Report not found.\",\n",
        "    file_name=\"vnv_report.md\",\n",
        "    key=\"download_vnv\"\n",
        "    )\n",
        "\"\"\"\n",
        "Path(\"app/app.py\").write_text(APP_PY, encoding=\"utf-8\")\n",
        "print(\"Wrote app/app.py\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys, subprocess\n",
        "\n",
        "port = os.environ.get(\"PORT\", \"8501\")\n",
        "address = os.environ.get(\"BIND_ADDR\", \"localhost\")  # ensure clickable link uses localhost\n",
        "cmd = [\n",
        "    sys.executable, \"-m\", \"streamlit\", \"run\", \"app/app.py\",\n",
        "    \"--server.address\", address,\n",
        "    \"--server.port\", str(port),\n",
        "    \"--browser.serverAddress\", \"localhost\",\n",
        "    \"--browser.serverPort\", str(port),\n",
        "]\n",
        "print(\"Launching:\", \" \".join(cmd))\n",
        "print(f\"Open: http://localhost:{port}/\")\n",
        "# This will hold the cell until you stop Streamlit (Ctrl+C in terminal/kernel).\n",
        "subprocess.run(cmd, check=False)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
